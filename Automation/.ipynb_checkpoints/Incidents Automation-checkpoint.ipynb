{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69ef45a-f53d-49a2-a2af-a9532253944e",
   "metadata": {},
   "source": [
    "### Installing Luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef3348-47d0-4810-8252-419d42fedcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install luigi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00960a-8dd2-4aa2-81ed-41819f90df07",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3904489-9c78-41f0-b237-f077abefd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import mysql.connector as mysql\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import luigi\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225850e5-8027-4ead-8917-dcc8f0138bad",
   "metadata": {},
   "source": [
    "### Define the connection parameters for MySQL and MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63894e14-4049-4296-8520-6d2eb7267cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWORD = 'sana123'\n",
    "MYSQL_DB = 'montgomery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185f4e06-72d7-41aa-8c5c-b741e9ba24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09b7ec-c258-424f-90d1-a0a5ae98f82c",
   "metadata": {},
   "source": [
    "### Importing API librariers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201e386d-6594-4926-a43e-d274139a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b7f25f-f12d-48ad-b376-67ec0e4320b3",
   "metadata": {},
   "source": [
    "### Define the task to extract data from the Socrata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decba58d-67e9-4bbe-b112-2bf2b32d5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSocrataDataJSON(luigi.Task):\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"incidents.json\")\n",
    "    \n",
    "    def run(self):\n",
    "        socrata_domain = 'data.montgomerycountymd.gov'\n",
    "        socrata_dataset_identifier_incidents = 'bhju-22kf'\n",
    "        socrata_token = os.environ.get(\"SODAPY_APPTOKEN\")\n",
    "        client = Socrata(socrata_domain, socrata_token)\n",
    "        results = client.get(socrata_dataset_identifier_incidents)\n",
    "        df = pd.DataFrame.from_dict(results)\n",
    "        incidents_data = df.to_json(orient='records')\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(incidents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543e63bc-4572-40ad-9c82-27a64b56736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSocrataDataCSV(luigi.Task):\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"incidents.csv\")  # Output CSV file\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Currently ExtractSocrataDataCSV is in progress\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        socrata_domain = 'data.montgomerycountymd.gov'\n",
    "        socrata_dataset_identifier_incidents = 'bhju-22kf'\n",
    "        socrata_token = os.environ.get(\"SODAPY_APPTOKEN\")\n",
    "        client = Socrata(socrata_domain, socrata_token)\n",
    "        results = client.get(socrata_dataset_identifier_incidents)\n",
    "        df = pd.DataFrame.from_dict(results)\n",
    "        #df.head()\n",
    "        # Use the filter method to select columns that don't start with \":@\"\n",
    "        filtered_columns = df.filter(regex=\"^:@\", axis=1)\n",
    "        columns_to_drop = ['latitude', 'longitude','geolocation']\n",
    "        # Drop the selected columns\n",
    "        df.drop(filtered_columns, axis=1, inplace=True)\n",
    "        df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        df.to_csv(self.output().path, index=False)  # Save data to CSV file\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"ExtractSocrataDataCSV Finished Successfully\")\n",
    "        print(\"-----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732131a-aa5c-4acf-8c15-551483b3c519",
   "metadata": {},
   "source": [
    "### Define the task to load data into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eba9c8a-76a7-4d08-904a-56fc62007d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadMySQLData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return ExtractSocrataDataCSV()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"sql_frame.csv\")  # Output file\n",
    "        \n",
    "    def run(self):\n",
    "            # Define the MySQL connection parameters\n",
    "            host = 'localhost'\n",
    "            user = 'root'\n",
    "            password = 'sana123'\n",
    "            database = 'montgomery2'\n",
    "        \n",
    "            # Define the SQL queries\n",
    "            create_database_query = f\"CREATE DATABASE IF NOT EXISTS {database}\"\n",
    "            use_database_query = f\"USE {database}\"\n",
    "            create_table_query = '''CREATE TABLE IF NOT EXISTS incidents (\n",
    "                                report_number TEXT,\n",
    "                                local_case_number TEXT,\n",
    "                                agency_name TEXT,\n",
    "                                acrs_report_type TEXT,\n",
    "                                crash_date_time TEXT,\n",
    "                                hit_run TEXT,\n",
    "                                lane_number TEXT,\n",
    "                                number_of_lanes TEXT,\n",
    "                                non_traffic TEXT,\n",
    "                                off_road_description TEXT,\n",
    "                                at_fault TEXT,\n",
    "                                collision_type TEXT,\n",
    "                                weather TEXT,\n",
    "                                light TEXT,\n",
    "                                traffic_control TEXT,\n",
    "                                driver_substance_abuse TEXT,\n",
    "                                first_harmful_event TEXT,\n",
    "                                second_harmful_event TEXT,\n",
    "                                fixed_object_struck TEXT,\n",
    "                                route_type TEXT,\n",
    "                                mile_point TEXT,\n",
    "                                mile_point_direction TEXT,\n",
    "                                lane_direction TEXT,\n",
    "                                direction TEXT,\n",
    "                                distance TEXT,\n",
    "                                distance_unit TEXT,\n",
    "                                road_grade TEXT,\n",
    "                                road_name TEXT,\n",
    "                                cross_street_type TEXT,\n",
    "                                cross_street_name TEXT,\n",
    "                                municipality TEXT,\n",
    "                                surface_condition TEXT,\n",
    "                                junction TEXT,\n",
    "                                intersection_type TEXT,\n",
    "                                intersection_area TEXT,\n",
    "                                road_alignment TEXT,\n",
    "                                road_condition TEXT,\n",
    "                                road_division TEXT,\n",
    "                                related_non_motorist TEXT,\n",
    "                                non_motorist_substance_abuse TEXT,\n",
    "                                lane_type TEXT\n",
    "                                )'''\n",
    "            show_table_query = \"SHOW TABLES\"\n",
    "            drop_columns_query = '''ALTER TABLE incidents\n",
    "                                DROP COLUMN latitude,\n",
    "                                DROP COLUMN longitude,\n",
    "                                DROP COLUMN location'''\n",
    "            insert_data_query = '''INSERT INTO incidents (report_number, local_case_number, agency_name, acrs_report_type, crash_date_time, hit_run, lane_number, number_of_lanes, non_traffic, off_road_description, at_fault,collision_type, weather, light, traffic_control, driver_substance_abuse, first_harmful_event, second_harmful_event, fixed_object_struck, route_type, mile_point, mile_point_direction, lane_direction, direction, distance, distance_unit, road_grade, road_name, cross_street_type, cross_street_name, municipality, surface_condition, junction, intersection_type, intersection_area, road_alignment, road_condition, road_division,related_non_motorist,non_motorist_substance_abuse,lane_type) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Currently LoadMySQLData is in progress\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "            # Connect to MySQL database\n",
    "            conn = mysql.connect(host=host, user=user, password=password)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Create database if it does not exist\n",
    "            cursor.execute(create_database_query)\n",
    "\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Database created successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "            # Use the specified database\n",
    "            cursor.execute(use_database_query)\n",
    "\n",
    "            df = pd.read_csv(self.input().path)\n",
    "            #read csv file and create a table structure\n",
    "            #df = pd.read_csv('incidents.csv')\n",
    "            print(df.dtypes)\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            # Set default data type for columns\n",
    "            default_dtype = 'TEXT'\n",
    "            # Create column definitions for SQL table\n",
    "            columns = [f\"{col} {default_dtype}\" for col in df.columns]\n",
    "            table_name = 'incidents'\n",
    "            create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns)});\"\n",
    "\n",
    "            # Create table if it does not exist\n",
    "            cursor.execute(create_table_sql)\n",
    "\n",
    "            # Show tables in the database\n",
    "            cursor.execute(show_table_query)\n",
    "            tables = cursor.fetchall()\n",
    "            for table in tables:\n",
    "                print(table[0])\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            # Show columns in the table\n",
    "            cursor.execute(\"DESCRIBE incidents\")\n",
    "            columns = cursor.fetchall()\n",
    "            for column in columns:\n",
    "                print(column[0], \"-\", column[1])\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Construct the INSERT statement\n",
    "            table_name = 'incidents'\n",
    "            columns = ', '.join(df.columns)\n",
    "            placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "            #insert_statement = '''INSERT INTO incidents ({columns}) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "            insert_statement =f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "            # Insert data into the table\n",
    "            df_filled = df.fillna(\"Missing\")\n",
    "            df_filled.astype(str)\n",
    "            inserted_records_count = 0\n",
    "            for index, row in df_filled.iterrows():\n",
    "                #cursor.execute(insert_data_query, tuple(row))\n",
    "                cursor.execute(insert_statement, tuple(row))\n",
    "                inserted_records_count += 1\n",
    "            conn.commit()\n",
    "        \n",
    "            #print(\"Number of records inserted into the incidents table:\", inserted_records_count)\n",
    "        \n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Records inserted successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Check the count of inserted data\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM incidents\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(\"Number of records in 'incidents' table:\", count)\n",
    "\n",
    "            cursor.close()\n",
    "            #conn.close()\n",
    "        \n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"LoadMySQLData finished successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Define the SQL query to read data\n",
    "            read_data_query = \"SELECT * FROM montgomery2.incidents\"\n",
    "\n",
    "            # Connect to MySQL database\n",
    "            #conn = mysql.connect(host=host, user=user, password=password, database=database)\n",
    "        \n",
    "            #Read data from MySQL into a DataFrame\n",
    "            sql_frame = pd.read_sql(read_data_query, conn)\n",
    "            sql_frame.to_csv(self.output().path, index=False)\n",
    "            conn.close()\n",
    "            #return sql_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40136a8c-d7c2-43ec-92eb-3c701f53611a",
   "metadata": {},
   "source": [
    "### Define the transformation task using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b8e416-3649-41de-965c-1d3ca2ce4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return LoadMySQLData()\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"transformed_data.json\")  # Output file\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"TransformData is in progress and reading data from MySQL\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "        # #Define the MySQL connection parameters\n",
    "        # host = 'localhost'\n",
    "        # user = 'root'\n",
    "        # password = 'sana123'\n",
    "        # database = 'montgomery2'\n",
    "\n",
    "        # # Define the SQL query to read data\n",
    "        # read_data_query = \"SELECT * FROM montgomery2.incidents\"\n",
    "\n",
    "        # # Connect to MySQL database\n",
    "        # conn = mysql.connect(host=host, user=user, password=password, database=database)\n",
    "        \n",
    "        # # Read data from MySQL into a DataFrame\n",
    "        # sql_frame = pd.read_sql(read_data_query, conn)\n",
    "        # #sql_frame=self.input().run()\n",
    "        #df = pd.read_csv(self.input().path)\n",
    "        sql_frame=pd.read_csv(self.input().path)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Get the total number of records\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        num_rows, num_columns = sql_frame.shape\n",
    "        print(\"Number of rows:\", num_rows)\n",
    "        print(\"Number of columns:\", num_columns)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Checking for Duplicate records\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        # Checking for duplicate records\n",
    "        duplicate_rows = sql_frame.duplicated()\n",
    "        print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Checking for 'Missing' Values\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        # Check where \"Missing\" values occur in each column\n",
    "        missing_mask = sql_frame.eq('Missing')\n",
    "        # Check which columns have at least one \"Missing\" value\n",
    "        columns_with_missing = missing_mask.any()\n",
    "        # Extract the column names where \"Missing\" values occur\n",
    "        columns_with_missing_values = columns_with_missing[columns_with_missing].index.tolist()\n",
    "        # Print the columns with \"Missing\" values\n",
    "        print(\"Columns with 'Missing' values:\", columns_with_missing_values)\n",
    "\n",
    "        # Iterate through columns with missing values\n",
    "        for column in columns_with_missing_values:\n",
    "            # Get unique values and their counts\n",
    "            unique_values_counts = sql_frame[column].value_counts() \n",
    "            # Print column name\n",
    "            print(\"Column:\", column)\n",
    "            # Print unique values and their counts\n",
    "            print(unique_values_counts)\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "\n",
    "        # Drop specified columns\n",
    "        columns_to_drop = ['off_road_description', 'first_harmful_event', 'related_non_motorist','second_harmful_event', 'fixed_oject_struck', 'cross_street_name','non_motorist_substance_abuse','lane_type','mile_point_direction', 'intersection_area', 'road_division']\n",
    "        sql_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        # Print the remaining columns\n",
    "        print(\"Remaining columns after dropping:\")\n",
    "        print(sql_frame.columns.tolist())\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        #Transformations\n",
    "        \n",
    "        # Save the transformed DataFrame to JSON\n",
    "        #filename = \"transformed_data.json\"\n",
    "        transformed_frame=sql_frame\n",
    "        transformed_frame.to_json(self.output().path, orient='records')\n",
    "\n",
    "        # Close MySQL connection\n",
    "        #conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98773cd1-3d26-4341-81ed-78d664a451da",
   "metadata": {},
   "source": [
    "### Define the MongoDB task to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a51767f-f92d-4362-a8c6-0d4151b0beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadMongoDBData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return TransformData()\n",
    "    \n",
    "    def run(self):\n",
    "        # MongoDB connection URI\n",
    "        uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\"\n",
    "\n",
    "        # Create a new client and connect to the server\n",
    "        client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "        try:\n",
    "            # Ping the MongoDB deployment\n",
    "            client.montgomery.command('ping')\n",
    "            print(\"Pinged the MongoDB deployment. Successfully connected to MongoDB!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        try:\n",
    "            # Check server status\n",
    "            server_status = client.montgomery.command('serverStatus')\n",
    "            print(\"Server is up and running.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        # List databases\n",
    "        databases = client.list_database_names()\n",
    "        print(\"Databases:\")\n",
    "        for db_obj in databases:\n",
    "            print(db_obj)\n",
    "\n",
    "        # Select database and collection\n",
    "        database_name = \"montgomery\"\n",
    "        db = client[database_name]\n",
    "\n",
    "        # List collections in the selected database\n",
    "        collections = db.list_collection_names()\n",
    "        print(\"\\nCollections in\", database_name, \":\")\n",
    "        for col in collections:\n",
    "            print(col)\n",
    "\n",
    "        # Load JSON file\n",
    "        #filename = \"transformed_data.json\"\n",
    "        with open(self.input().path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Insert documents into collection\n",
    "        collection_name = 'incidents'  # assuming collection name is 'incidents'\n",
    "        collection = db[collection_name]\n",
    "        collection.insert_many(data)\n",
    "        print(\"JSON data successfully loaded into MongoDB collection 'incidents' in database 'montgomery'.\")\n",
    "\n",
    "        # Get the total number of documents in the collection\n",
    "        total_records = collection.count_documents({})\n",
    "        print(\"Total number of records in the collection:\", total_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d9772-046c-4eaa-ac79-8f4a7fec2197",
   "metadata": {},
   "source": [
    "### Define the task to fetch data from MongoDB into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c680bd3f-6a0e-4aa6-bf42-f92ea1527515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchMongoDBData(luigi.Task):\n",
    "    # def requires(self):\n",
    "    #     return LoadMongoDBData()\n",
    "        \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"fetched_data.csv\")  # Output file\n",
    "    \n",
    "    def run(self):\n",
    "        # MongoDB connection URI\n",
    "        uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\"\n",
    "        # Create a new client and connect to the server\n",
    "        client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "        db = client['montgomery']\n",
    "        collection = db['incidents']\n",
    "        \n",
    "        # Query all documents from the collection\n",
    "        data = list(collection.find())\n",
    "        \n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(self.output().path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24f63-ad0c-4a2c-9d95-9e9c7c5def24",
   "metadata": {},
   "source": [
    "### Define the main task to run the ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b8b8c2-972a-405d-9161-96827d37fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main task to run the ETL pipeline\n",
    "class ETLPipeline(luigi.Task):\n",
    "    def requires(self):\n",
    "        return FetchMongoDBData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d350464-ad5e-4e18-9dfc-0ba7dc1bc824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if ETLPipeline() is complete\n",
      "DEBUG: Checking if FetchMongoDBData() is complete\n",
      "INFO: Informed scheduler that task   ETLPipeline__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if LoadMongoDBData() is complete\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if TransformData() is complete\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   TransformData__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 3\n",
      "INFO: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   LoadMongoDBData()\n",
      "INFO: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) done      LoadMongoDBData()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   FetchMongoDBData()\n",
      "ERROR: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) failed    FetchMongoDBData()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py\", line 195, in run\n",
      "    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n",
      "RuntimeError: Unfulfilled dependency at run time: LoadMongoDBData__99914b932b\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   FAILED\n",
      "DEBUG: Checking if FetchMongoDBData() is complete\n",
      "DEBUG: Checking if LoadMongoDBData() is complete\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if TransformData() is complete\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   TransformData__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   FetchMongoDBData()\n",
      "ERROR: [pid 34396] Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) failed    FetchMongoDBData()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py\", line 195, in run\n",
      "    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n",
      "RuntimeError: Unfulfilled dependency at run time: LoadMongoDBData__99914b932b\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   FAILED\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "DEBUG: There are 2 pending tasks possibly being run by other workers\n",
      "DEBUG: There are 2 pending tasks unique to this worker\n",
      "DEBUG: There are 2 pending tasks last scheduled by this worker\n",
      "INFO: Worker Worker(salt=7650770777, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 TransformData()\n",
      "* 1 ran successfully:\n",
      "    - 1 LoadMongoDBData()\n",
      "* 1 failed:\n",
      "    - 1 FetchMongoDBData()\n",
      "* 1 were left pending, among these:\n",
      "    * 1 had failed dependencies:\n",
      "        - 1 ETLPipeline()\n",
      "\n",
      "This progress looks :( because there were failed tasks\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Configure logging\n",
    "    logging.basicConfig(filename='etl_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Redirect stdout and stderr to the log file\n",
    "    sys.stdout = open('etl_pipeline.log', 'a')\n",
    "    sys.stderr = open('etl_pipeline.log', 'a')\n",
    "\n",
    "    # Run the Luigi build process\n",
    "    try:\n",
    "        luigi.build([ETLPipeline()], local_scheduler=True)\n",
    "        logging.info(\"ETL pipeline executed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ETL pipeline execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c586cd-843e-4cbc-9704-f2ed04302a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if ETLPipeline() is complete\n",
      "C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py:426: UserWarning: Task ETLPipeline() without outputs has no custom complete() method\n",
      "  is_complete = task.complete()\n",
      "DEBUG: Checking if FetchMongoDBData() is complete\n",
      "INFO: Informed scheduler that task   ETLPipeline__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if LoadMongoDBData() is complete\n",
      "C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py:426: UserWarning: Task LoadMongoDBData() without outputs has no custom complete() method\n",
      "  is_complete = task.complete()\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if TransformData() is complete\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   TransformData__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 3\n",
      "INFO: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   LoadMongoDBData()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged the MongoDB deployment. Successfully connected to MongoDB!\n",
      "Server is up and running.\n",
      "Databases:\n",
      "montgomery\n",
      "montgomery_reverse\n",
      "admin\n",
      "local\n",
      "\n",
      "Collections in montgomery :\n",
      "drivers\n",
      "non_motorists\n",
      "non_motorists_dataset\n",
      "incidents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) done      LoadMongoDBData()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   FetchMongoDBData()\n",
      "ERROR: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) failed    FetchMongoDBData()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py\", line 195, in run\n",
      "    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n",
      "RuntimeError: Unfulfilled dependency at run time: LoadMongoDBData__99914b932b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully loaded into MongoDB collection 'incidents' in database 'montgomery'.\n",
      "Total number of records in the collection: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   FAILED\n",
      "DEBUG: Checking if FetchMongoDBData() is complete\n",
      "DEBUG: Checking if LoadMongoDBData() is complete\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if TransformData() is complete\n",
      "INFO: Informed scheduler that task   LoadMongoDBData__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   TransformData__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) running   FetchMongoDBData()\n",
      "ERROR: [pid 34396] Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) failed    FetchMongoDBData()\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SANA JALGAONKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\luigi\\worker.py\", line 195, in run\n",
      "    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n",
      "RuntimeError: Unfulfilled dependency at run time: LoadMongoDBData__99914b932b\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   FetchMongoDBData__99914b932b   has status   FAILED\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "DEBUG: There are 2 pending tasks possibly being run by other workers\n",
      "DEBUG: There are 2 pending tasks unique to this worker\n",
      "DEBUG: There are 2 pending tasks last scheduled by this worker\n",
      "INFO: Worker Worker(salt=715081680, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=34396) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 TransformData()\n",
      "* 1 ran successfully:\n",
      "    - 1 LoadMongoDBData()\n",
      "* 1 failed:\n",
      "    - 1 FetchMongoDBData()\n",
      "* 1 were left pending, among these:\n",
      "    * 1 had failed dependencies:\n",
      "        - 1 ETLPipeline()\n",
      "\n",
      "This progress looks :( because there were failed tasks\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     luigi.build([ETLPipeline()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe4a85-2df3-4c59-95de-6d226687618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyis_frame=pd.read_csv(\"fetched_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
