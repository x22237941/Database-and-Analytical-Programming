{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69ef45a-f53d-49a2-a2af-a9532253944e",
   "metadata": {},
   "source": [
    "### Installing Luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef3348-47d0-4810-8252-419d42fedcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install luigi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00960a-8dd2-4aa2-81ed-41819f90df07",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3904489-9c78-41f0-b237-f077abefd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import mysql.connector as mysql\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import luigi\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225850e5-8027-4ead-8917-dcc8f0138bad",
   "metadata": {},
   "source": [
    "### Define the connection parameters for MySQL and MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63894e14-4049-4296-8520-6d2eb7267cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWORD = 'sana123'\n",
    "MYSQL_DB = 'montgomery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185f4e06-72d7-41aa-8c5c-b741e9ba24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09b7ec-c258-424f-90d1-a0a5ae98f82c",
   "metadata": {},
   "source": [
    "### Importing API librariers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201e386d-6594-4926-a43e-d274139a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b7f25f-f12d-48ad-b376-67ec0e4320b3",
   "metadata": {},
   "source": [
    "### Define the task to extract data from the Socrata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decba58d-67e9-4bbe-b112-2bf2b32d5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSocrataDataJSON(luigi.Task):\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"incidents.json\")\n",
    "    \n",
    "    def run(self):\n",
    "        socrata_domain = 'data.montgomerycountymd.gov'\n",
    "        socrata_dataset_identifier_incidents = 'bhju-22kf'\n",
    "        socrata_token = os.environ.get(\"SODAPY_APPTOKEN\")\n",
    "        client = Socrata(socrata_domain, socrata_token)\n",
    "        results = client.get(socrata_dataset_identifier_incidents)\n",
    "        df = pd.DataFrame.from_dict(results)\n",
    "        incidents_data = df.to_json(orient='records')\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(incidents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543e63bc-4572-40ad-9c82-27a64b56736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSocrataDataCSV(luigi.Task):\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"incidents.csv\")  # Output CSV file\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Currently ExtractSocrataDataCSV is in progress\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        socrata_domain = 'data.montgomerycountymd.gov'\n",
    "        socrata_dataset_identifier_incidents = 'bhju-22kf'\n",
    "        socrata_token = os.environ.get(\"SODAPY_APPTOKEN\")\n",
    "        client = Socrata(socrata_domain, socrata_token)\n",
    "        results = client.get(socrata_dataset_identifier_incidents)\n",
    "        df = pd.DataFrame.from_dict(results)\n",
    "        #df.head()\n",
    "        # Use the filter method to select columns that don't start with \":@\"\n",
    "        filtered_columns = df.filter(regex=\"^:@\", axis=1)\n",
    "        columns_to_drop = ['latitude', 'longitude','geolocation']\n",
    "        # Drop the selected columns\n",
    "        df.drop(filtered_columns, axis=1, inplace=True)\n",
    "        df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        df.to_csv(self.output().path, index=False)  # Save data to CSV file\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"ExtractSocrataDataCSV Finished Successfully\")\n",
    "        print(\"-----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732131a-aa5c-4acf-8c15-551483b3c519",
   "metadata": {},
   "source": [
    "### Define the task to load data into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eba9c8a-76a7-4d08-904a-56fc62007d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadMySQLData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return ExtractSocrataDataCSV()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"sql_frame.csv\")  # Output file\n",
    "        \n",
    "    def run(self):\n",
    "            # Define the MySQL connection parameters\n",
    "            host = 'localhost'\n",
    "            user = 'root'\n",
    "            password = 'sana123'\n",
    "            database = 'montgomery2'\n",
    "        \n",
    "            # Define the SQL queries\n",
    "            create_database_query = f\"CREATE DATABASE IF NOT EXISTS {database}\"\n",
    "            use_database_query = f\"USE {database}\"\n",
    "            create_table_query = '''CREATE TABLE IF NOT EXISTS incidents (\n",
    "                                report_number TEXT,\n",
    "                                local_case_number TEXT,\n",
    "                                agency_name TEXT,\n",
    "                                acrs_report_type TEXT,\n",
    "                                crash_date_time TEXT,\n",
    "                                hit_run TEXT,\n",
    "                                lane_number TEXT,\n",
    "                                number_of_lanes TEXT,\n",
    "                                non_traffic TEXT,\n",
    "                                off_road_description TEXT,\n",
    "                                at_fault TEXT,\n",
    "                                collision_type TEXT,\n",
    "                                weather TEXT,\n",
    "                                light TEXT,\n",
    "                                traffic_control TEXT,\n",
    "                                driver_substance_abuse TEXT,\n",
    "                                first_harmful_event TEXT,\n",
    "                                second_harmful_event TEXT,\n",
    "                                fixed_object_struck TEXT,\n",
    "                                route_type TEXT,\n",
    "                                mile_point TEXT,\n",
    "                                mile_point_direction TEXT,\n",
    "                                lane_direction TEXT,\n",
    "                                direction TEXT,\n",
    "                                distance TEXT,\n",
    "                                distance_unit TEXT,\n",
    "                                road_grade TEXT,\n",
    "                                road_name TEXT,\n",
    "                                cross_street_type TEXT,\n",
    "                                cross_street_name TEXT,\n",
    "                                municipality TEXT,\n",
    "                                surface_condition TEXT,\n",
    "                                junction TEXT,\n",
    "                                intersection_type TEXT,\n",
    "                                intersection_area TEXT,\n",
    "                                road_alignment TEXT,\n",
    "                                road_condition TEXT,\n",
    "                                road_division TEXT,\n",
    "                                related_non_motorist TEXT,\n",
    "                                non_motorist_substance_abuse TEXT,\n",
    "                                lane_type TEXT\n",
    "                                )'''\n",
    "            show_table_query = \"SHOW TABLES\"\n",
    "            drop_columns_query = '''ALTER TABLE incidents\n",
    "                                DROP COLUMN latitude,\n",
    "                                DROP COLUMN longitude,\n",
    "                                DROP COLUMN location'''\n",
    "            insert_data_query = '''INSERT INTO incidents (report_number, local_case_number, agency_name, acrs_report_type, crash_date_time, hit_run, lane_number, number_of_lanes, non_traffic, off_road_description, at_fault,collision_type, weather, light, traffic_control, driver_substance_abuse, first_harmful_event, second_harmful_event, fixed_object_struck, route_type, mile_point, mile_point_direction, lane_direction, direction, distance, distance_unit, road_grade, road_name, cross_street_type, cross_street_name, municipality, surface_condition, junction, intersection_type, intersection_area, road_alignment, road_condition, road_division,related_non_motorist,non_motorist_substance_abuse,lane_type) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Currently LoadMySQLData is in progress\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "            # Connect to MySQL database\n",
    "            conn = mysql.connect(host=host, user=user, password=password)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Create database if it does not exist\n",
    "            cursor.execute(create_database_query)\n",
    "\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Database created successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "            # Use the specified database\n",
    "            cursor.execute(use_database_query)\n",
    "\n",
    "            df = pd.read_csv(self.input().path)\n",
    "            #read csv file and create a table structure\n",
    "            #df = pd.read_csv('incidents.csv')\n",
    "            print(df.dtypes)\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            # Set default data type for columns\n",
    "            default_dtype = 'TEXT'\n",
    "            # Create column definitions for SQL table\n",
    "            columns = [f\"{col} {default_dtype}\" for col in df.columns]\n",
    "            table_name = 'incidents'\n",
    "            create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns)});\"\n",
    "\n",
    "            # Create table if it does not exist\n",
    "            cursor.execute(create_table_sql)\n",
    "\n",
    "            # Show tables in the database\n",
    "            cursor.execute(show_table_query)\n",
    "            tables = cursor.fetchall()\n",
    "            for table in tables:\n",
    "                print(table[0])\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            # Show columns in the table\n",
    "            cursor.execute(\"DESCRIBE incidents\")\n",
    "            columns = cursor.fetchall()\n",
    "            for column in columns:\n",
    "                print(column[0], \"-\", column[1])\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Construct the INSERT statement\n",
    "            table_name = 'incidents'\n",
    "            columns = ', '.join(df.columns)\n",
    "            placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "            #insert_statement = '''INSERT INTO incidents ({columns}) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "            insert_statement =f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "            # Insert data into the table\n",
    "            df_filled = df.fillna(\"Missing\")\n",
    "            df_filled.astype(str)\n",
    "            inserted_records_count = 0\n",
    "            for index, row in df_filled.iterrows():\n",
    "                #cursor.execute(insert_data_query, tuple(row))\n",
    "                cursor.execute(insert_statement, tuple(row))\n",
    "                inserted_records_count += 1\n",
    "            conn.commit()\n",
    "        \n",
    "            #print(\"Number of records inserted into the incidents table:\", inserted_records_count)\n",
    "        \n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"Records inserted successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Check the count of inserted data\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM incidents\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(\"Number of records in 'incidents' table:\", count)\n",
    "\n",
    "            cursor.close()\n",
    "            #conn.close()\n",
    "        \n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "            print(\"LoadMySQLData finished successfully\")\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            #Define the SQL query to read data\n",
    "            read_data_query = \"SELECT * FROM montgomery2.incidents\"\n",
    "\n",
    "            # Connect to MySQL database\n",
    "            #conn = mysql.connect(host=host, user=user, password=password, database=database)\n",
    "        \n",
    "            #Read data from MySQL into a DataFrame\n",
    "            sql_frame = pd.read_sql(read_data_query, conn)\n",
    "            sql_frame.to_csv(self.output().path, index=False)\n",
    "            conn.close()\n",
    "            #return sql_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40136a8c-d7c2-43ec-92eb-3c701f53611a",
   "metadata": {},
   "source": [
    "### Define the transformation task using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b8e416-3649-41de-965c-1d3ca2ce4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return LoadMySQLData()\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"transformed_data.json\")  # Output file\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"TransformData is in progress and reading data from MySQL\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "        # #Define the MySQL connection parameters\n",
    "        # host = 'localhost'\n",
    "        # user = 'root'\n",
    "        # password = 'sana123'\n",
    "        # database = 'montgomery2'\n",
    "\n",
    "        # # Define the SQL query to read data\n",
    "        # read_data_query = \"SELECT * FROM montgomery2.incidents\"\n",
    "\n",
    "        # # Connect to MySQL database\n",
    "        # conn = mysql.connect(host=host, user=user, password=password, database=database)\n",
    "        \n",
    "        # # Read data from MySQL into a DataFrame\n",
    "        # sql_frame = pd.read_sql(read_data_query, conn)\n",
    "        # #sql_frame=self.input().run()\n",
    "        #df = pd.read_csv(self.input().path)\n",
    "        sql_frame=pd.read_csv(self.input().path)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Get the total number of records\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        num_rows, num_columns = sql_frame.shape\n",
    "        print(\"Number of rows:\", num_rows)\n",
    "        print(\"Number of columns:\", num_columns)\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Checking for Duplicate records\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        # Checking for duplicate records\n",
    "        duplicate_rows = sql_frame.duplicated()\n",
    "        print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"Checking for 'Missing' Values\")\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        # Check where \"Missing\" values occur in each column\n",
    "        missing_mask = sql_frame.eq('Missing')\n",
    "        # Check which columns have at least one \"Missing\" value\n",
    "        columns_with_missing = missing_mask.any()\n",
    "        # Extract the column names where \"Missing\" values occur\n",
    "        columns_with_missing_values = columns_with_missing[columns_with_missing].index.tolist()\n",
    "        # Print the columns with \"Missing\" values\n",
    "        print(\"Columns with 'Missing' values:\", columns_with_missing_values)\n",
    "\n",
    "        # Iterate through columns with missing values\n",
    "        for column in columns_with_missing_values:\n",
    "            # Get unique values and their counts\n",
    "            unique_values_counts = sql_frame[column].value_counts() \n",
    "            # Print column name\n",
    "            print(\"Column:\", column)\n",
    "            # Print unique values and their counts\n",
    "            print(unique_values_counts)\n",
    "            print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "\n",
    "        # Drop specified columns\n",
    "        columns_to_drop = ['off_road_description', 'first_harmful_event', 'related_non_motorist','second_harmful_event', 'fixed_oject_struck', 'cross_street_name','non_motorist_substance_abuse','lane_type','mile_point_direction', 'intersection_area', 'road_division']\n",
    "        sql_frame.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        # Print the remaining columns\n",
    "        print(\"Remaining columns after dropping:\")\n",
    "        print(sql_frame.columns.tolist())\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        #Transformations\n",
    "        \n",
    "        # Save the transformed DataFrame to JSON\n",
    "        #filename = \"transformed_data.json\"\n",
    "        transformed_frame=sql_frame\n",
    "        transformed_frame.to_json(self.output().path, orient='records')\n",
    "\n",
    "        # Close MySQL connection\n",
    "        #conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98773cd1-3d26-4341-81ed-78d664a451da",
   "metadata": {},
   "source": [
    "### Define the MongoDB task to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a51767f-f92d-4362-a8c6-0d4151b0beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadMongoDBData(luigi.Task):\n",
    "    def requires(self):\n",
    "        return TransformData()\n",
    "    \n",
    "    def run(self):\n",
    "        # MongoDB connection URI\n",
    "        uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\"\n",
    "\n",
    "        # Create a new client and connect to the server\n",
    "        client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "        try:\n",
    "            # Ping the MongoDB deployment\n",
    "            client.montgomery.command('ping')\n",
    "            print(\"Pinged the MongoDB deployment. Successfully connected to MongoDB!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        try:\n",
    "            # Check server status\n",
    "            server_status = client.montgomery.command('serverStatus')\n",
    "            print(\"Server is up and running.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        # List databases\n",
    "        databases = client.list_database_names()\n",
    "        print(\"Databases:\")\n",
    "        for db_obj in databases:\n",
    "            print(db_obj)\n",
    "\n",
    "        # Select database and collection\n",
    "        database_name = \"montgomery\"\n",
    "        db = client[database_name]\n",
    "\n",
    "        # List collections in the selected database\n",
    "        collections = db.list_collection_names()\n",
    "        print(\"\\nCollections in\", database_name, \":\")\n",
    "        for col in collections:\n",
    "            print(col)\n",
    "\n",
    "        # Load JSON file\n",
    "        #filename = \"transformed_data.json\"\n",
    "        with open(self.input().path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Insert documents into collection\n",
    "        collection_name = 'incidents'  # assuming collection name is 'incidents'\n",
    "        collection = db[collection_name]\n",
    "        collection.insert_many(data)\n",
    "        print(\"JSON data successfully loaded into MongoDB collection 'incidents' in database 'montgomery'.\")\n",
    "\n",
    "        # Get the total number of documents in the collection\n",
    "        total_records = collection.count_documents({})\n",
    "        print(\"Total number of records in the collection:\", total_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d9772-046c-4eaa-ac79-8f4a7fec2197",
   "metadata": {},
   "source": [
    "### Define the task to fetch data from MongoDB into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c680bd3f-6a0e-4aa6-bf42-f92ea1527515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetchMongoDBData(luigi.Task):\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"fetched_data.csv\")  # Output file\n",
    "    \n",
    "    def run(self):\n",
    "        # MongoDB connection URI\n",
    "        uri = \"mongodb+srv://x22237941:sana123@montgomerycluster.tzxvtsd.mongodb.net/?retryWrites=true&w=majority&appName=montgomerycluster\"\n",
    "        # Create a new client and connect to the server\n",
    "        client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "        db = client['montgomery']\n",
    "        collection = db['incidents']\n",
    "        \n",
    "        # Query all documents from the collection\n",
    "        data = list(collection.find())\n",
    "        \n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(self.output().path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24f63-ad0c-4a2c-9d95-9e9c7c5def24",
   "metadata": {},
   "source": [
    "### Define the main task to run the ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8b8c2-972a-405d-9161-96827d37fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main task to run the ETL pipeline\n",
    "class ETLPipeline(luigi.Task):\n",
    "    def requires(self):\n",
    "        return LoadMongoDBData()\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"etl_pipeline.log\")  # Log file\n",
    "    def run(self):\n",
    "        with self.output().open('w') as f:\n",
    "            try:\n",
    "                # Run the task to load data into MongoDB\n",
    "                yield LoadMongoDBData()\n",
    "                f.write(\"Data loaded into MongoDB successfully.\\n\")\n",
    "                # Run the task to fetch data from MongoDB into a DataFrame\n",
    "                yield FetchMongoDBData()\n",
    "                f.write(\"Data fetched from MongoDB and saved as CSV successfully.\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5b433e-ea86-4461-a35d-85245ce9f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main task to run the ETL pipeline\n",
    "class ETLPipeline(luigi.Task):\n",
    "    def requires(self):\n",
    "        return LoadMongoDBData()\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"etl_pipeline.log\")  # Log file\n",
    "    \n",
    "    def run(self):\n",
    "        with self.output().open('w') as f:\n",
    "            try:\n",
    "                # Run the task to load data into MongoDB\n",
    "                yield LoadMongoDBData()\n",
    "                f.write(\"Data loaded into MongoDB successfully.\\n\")\n",
    "                # Run the task to fetch data from MongoDB into a DataFrame\n",
    "                yield FetchMongoDBData()\n",
    "                f.write(\"Data fetched from MongoDB and saved as CSV successfully.\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c586cd-843e-4cbc-9704-f2ed04302a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if ETLPipeline() is complete\n",
      "INFO: Informed scheduler that task   ETLPipeline__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=830968022, workers=1, host=LAPTOP-JCBDQEO9, username=SANA JALGAONKAR, pid=15740) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 ETLPipeline()\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    luigi.build([ETLPipeline()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe4a85-2df3-4c59-95de-6d226687618d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fetched_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m analyis_frame\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfetched_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fetched_data.csv'"
     ]
    }
   ],
   "source": [
    "analyis_frame=pd.read_csv(\"fetched_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
